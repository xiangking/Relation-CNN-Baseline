{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Complie\\Anconda\\envs\\TF\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# -*- coding:utf8 -*-\n",
    "\"\"\"\n",
    "该模型主要用于将文本转化为id形式，同时提供分词和训练词向量的静态函数\n",
    "\"\"\"\n",
    "\n",
    "import jieba\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from zhon.hanzi import punctuation\n",
    "from gensim.models import Word2Vec\n",
    "from functools import lru_cache\n",
    "\n",
    "class Vocab(object):\n",
    "    \n",
    "    def __init__(self, filename=None,  initial_tokens=None):\n",
    "        self.id2token = {}\n",
    "        self.token2id = {}\n",
    "\n",
    "        self.embed_dim = None\n",
    "        self.embeddings = None\n",
    "\n",
    "        self.pad_token = '<blank>'\n",
    "        self.unk_token = '<unk>'\n",
    "        \n",
    "        self.initial_tokens = initial_tokens.copy() if initial_tokens is not None else []\n",
    "        self.initial_tokens.insert(0, self.unk_token)\n",
    "        self.initial_tokens.insert(0, self.pad_token)\n",
    "        \n",
    "        for token in self.initial_tokens:\n",
    "            self.add(token)\n",
    "            \n",
    "            \n",
    "    def add(self, token) -> int:\n",
    "        \"\"\"\n",
    "        Adds the token to vocab\n",
    "        Args:\n",
    "            token: a string\n",
    "        \"\"\"\n",
    "        if token in self.token2id:\n",
    "            idx = self.token2id[token]\n",
    "        else:\n",
    "            idx = len(self.id2token)\n",
    "            self.id2token[idx] = token\n",
    "            self.token2id[token] = idx\n",
    "\n",
    "        return idx \n",
    "    \n",
    "    @property\n",
    "    @lru_cache(1)\n",
    "    def size(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the size of vocabulary\n",
    "        Returns:\n",
    "            an integer indicating the size\n",
    "        \"\"\"\n",
    "        return len(self.id2token)\n",
    "    \n",
    "    def get_id(self, token) -> int:\n",
    "        \"\"\"\n",
    "        Gets the id of a token, returns the id of unk token if token is not in vocab\n",
    "        Args:\n",
    "            key: a string indicating the word\n",
    "        Returns:\n",
    "            an integer\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self.token2id[token]\n",
    "        except KeyError:\n",
    "            return self.token2id[self.unk_token]\n",
    "        \n",
    "    def get_token(self, idx) -> str:\n",
    "        \"\"\"\n",
    "        Gets the token corresponding to idx, returns unk token if idx is not in vocab\n",
    "        Args:\n",
    "            idx: an integer\n",
    "        returns:\n",
    "            a token string\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self.id2token[idx]\n",
    "        except KeyError:\n",
    "            return self.unk_token\n",
    "        \n",
    "    def convert_to_ids(self, tokens) -> list:\n",
    "        \"\"\"\n",
    "        Convert a list of tokens to ids, use unk_token if the token is not in vocab.\n",
    "        Args:\n",
    "            tokens: a list of token\n",
    "        Returns:\n",
    "            a list of ids\n",
    "        \"\"\"\n",
    "        vec = [self.get_id(term) for term in tokens]\n",
    "        return vec\n",
    "    \n",
    "    def recover_from_ids(self, ids, stop_id=None) -> list:\n",
    "        \"\"\"\n",
    "        Convert a list of ids to tokens, stop converting if the stop_id is encountered\n",
    "        Args:\n",
    "            ids: a list of ids to convert\n",
    "            stop_id: the stop id, default is None\n",
    "        Returns:\n",
    "            a list of tokens\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        for i in ids:\n",
    "            tokens += [self.get_token(i)]\n",
    "            if stop_id is not None and i == stop_id:\n",
    "                break\n",
    "        return tokens\n",
    "    \n",
    "    def recover_id2token(self) -> dict:\n",
    "        \"\"\"\n",
    "        Rebuild the id2token\n",
    "        Returns:\n",
    "            a dict about converting id to token\n",
    "        \"\"\"\n",
    "        id2token_temp = {}\n",
    "        for token_iter, idx_iter in self.token2id:\n",
    "            id2token_temp[idx_iter] = token_iter\n",
    "            \n",
    "        return id2token_temp\n",
    "                    \n",
    "    \n",
    "    def load_pretrained_embeddings(self, trained_embeddings):\n",
    "        \"\"\"\n",
    "        Loads the pretrained embeddings\n",
    "        Args:\n",
    "            trained_embeddings: the pretrained embeddings\n",
    "        \"\"\"        \n",
    "        if self.embed_dim is None:\n",
    "            self.embed_dim = len(trained_embeddings[0])\n",
    "\n",
    "        # load embeddings\n",
    "        self.embeddings = np.zeros([self.size, self.embed_dim])\n",
    "        for idx, trained_vec in enumerate(trained_embeddings):\n",
    "            self.embeddings[idx+2] = trained_vec \n",
    "            \n",
    "\n",
    "    def randomly_init_embeddings(self, embed_dim):\n",
    "        \"\"\"\n",
    "        Randomly initializes the embeddings for each token\n",
    "        Args:\n",
    "            embed_dim: the size of the embedding for each token\n",
    "        \"\"\"\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embeddings = np.random.rand(self.size(), embed_dim)\n",
    "        for token in [self.pad_token, self.unk_token]:\n",
    "            self.embeddings[self.get_id(token)] = np.zeros([self.embed_dim])\n",
    "            \n",
    "    \n",
    "    def save(self, mode='pkl', base_dir = '.'):\n",
    "        \"\"\"\n",
    "        Save the dict and embedding\n",
    "        Args:\n",
    "            mode: the way to save\n",
    "            base_dir: the root path of the save file\n",
    "        \"\"\"\n",
    "        print(\"保存字典和词向量...........\\n\")\n",
    "        model_dir = f'{base_dir}/model'\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "            \n",
    "        if mode == 'pkl':\n",
    "            with open(f'{model_dir}/word_idx_dict.pkl', 'wb') as f:\n",
    "                pickle.dump(self.token2id , f)\n",
    "            \n",
    "            with open(f'{model_dir}/word_vectors_arr.pkl', 'wb') as f:\n",
    "                pickle.dump(self.embeddings, f)\n",
    "        \n",
    "        if mode == 'txt':\n",
    "            with open(f'{model_dir}/word_idx_dict.pkl', 'wb') as f:\n",
    "                for token_iter, id_iter in self.token2id.items():\n",
    "                    f.write(token_iter + ' ' + id_iter + '\\n')\n",
    "                    \n",
    "            with open(f'{model_dir}/word_idx_dict.pkl', 'wb') as f:\n",
    "                for embedding_vec_iter in self.embeddings:\n",
    "                    f.write(embedding_vec_iter + '\\n')\n",
    "                \n",
    "        print(\"字典和词向量已保存到该目录下！\\n\")\n",
    "        \n",
    "        \n",
    "    def load(self, mode='pkl', base_dir = '.'):\n",
    "        \"\"\"\n",
    "        Loads the dict and embedding \n",
    "        Args:\n",
    "            mode: the way to save\n",
    "            base_dir: the root path of the load file\n",
    "        \"\"\"\n",
    "        print(\"加载字典和词向量...........\\n\")\n",
    "                \n",
    "        model_dir = f'{base_dir}/model'\n",
    "            \n",
    "        if mode == 'pkl':\n",
    "            with open(f'{model_dir}/word_idx_dict.pkl', 'rb') as f:\n",
    "                self.token2id = pickle.load(f)\n",
    "            \n",
    "            self.id2token = self.recover_id2token()\n",
    "            \n",
    "                \n",
    "            with open(f'{model_dir}/word_vectors_arr.pkl', 'rb') as f:\n",
    "                self.embeddings = pickle.load(f)\n",
    "        # TO DO        \n",
    "        if mode == 'txt':\n",
    "            pass\n",
    "                \n",
    "        print(\"字典和词向量已保存到该目录下！\\n\")\n",
    "\n",
    "             \n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenize(text, filter_stop_word = None, lower=False) -> list:\n",
    "        \"\"\"\n",
    "        The function to tokenize\n",
    "        Args:\n",
    "            filter_stop_word: the stop words list\n",
    "            lower: lower or not\n",
    "        \"\"\"\n",
    "        text = re.sub(r'[%s]+' % punctuation, '', text) \n",
    "        if lower:\n",
    "            text = text.lower()\n",
    "        tokens = jieba.lcut(text)\n",
    "        if filter_stop_word:\n",
    "            stop_word_set = set(filter_stop_word)\n",
    "            tokens = filter(lambda w: w not in stop_word_set, tokens)\n",
    "        return list(tokens)\n",
    "    \n",
    "    @staticmethod\n",
    "    def train_word2vec(sentences, base_dir = '.'):\n",
    "        \"\"\"\n",
    "        Train a word2vec model\n",
    "        Args:\n",
    "            sentences: the train data\n",
    "            base_dir: the root path of the load file\n",
    "        \"\"\"\n",
    "        time_s = time.time()\n",
    "        vec_size = 300\n",
    "        win_size = 1\n",
    "        print ('begin to train model...')\n",
    "        w2v_model = Word2Vec(sentences=sentences, \n",
    "                             size=50, \n",
    "                             window=1, \n",
    "                             min_count=1,\n",
    "                             workers=20, \n",
    "                             sg=1, \n",
    "                             iter=25, \n",
    "                             hs=0)\n",
    "        print(\"train model success\\n\")\n",
    "\n",
    "        word2vec_dir = f'{base_dir}/word2vec'\n",
    "        if not os.path.exists(word2vec_dir):\n",
    "            os.makedirs(word2vec_dir)\n",
    "        \n",
    "        w2v_model.save(f'{word2vec_dir}/word2vec.model')\n",
    "        print ('save model success, model_path=%s, time=%.4f sec.' \n",
    "                % (f'{word2vec_dir}/word2vec.model', time.time() - time_s))\n",
    "        \n",
    "        return w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class RCDataset(object):\n",
    "    \"\"\"\n",
    "    This module implements the APIs for loading and using  dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train_file=[], dev_file=[], test_file=[], pad_id=0):\n",
    "\n",
    "        self.logger = logging.getLogger(\"RC\")\n",
    "\n",
    "        self.pattern_symbol = re.compile(r'[\\(\\)\\[\\]\\{\\},:;!~@^_$¥`<>]')\n",
    "\n",
    "        self.pad_id = pad_id\n",
    "        self.limit = 60\n",
    "\n",
    "        self.train_contents = []\n",
    "        self.dev_contents = []\n",
    "        self.test_contents = []\n",
    "\n",
    "        self.train_entities1_position = []\n",
    "        self.dev_entities1_position = []\n",
    "        self.test_entities1_position = []\n",
    "\n",
    "        self.train_entities2_position = []\n",
    "        self.dev_entities2_position = []\n",
    "        self.test_entities2_position = []\n",
    "\n",
    "        self.train_labels = []\n",
    "        self.dev_labels = []\n",
    "        self.test_labels = []\n",
    "\n",
    "        if train_file:\n",
    "            self.train_contents, self.train_entities1_position, self.train_entities2_position, self.train_labels = self._load_dataset(\n",
    "                train_file)\n",
    "            self.logger.info('Train set size: {} titles.'.format(\n",
    "                len(self.train_labels)))\n",
    "\n",
    "        if dev_file:\n",
    "            self.dev_contents, self.dev_entities1_position, self.dev_entities2_position, self.dev_labels = self._load_dataset(\n",
    "                dev_file)\n",
    "            self.logger.info('Dev set size: {} titles.'.format(\n",
    "                len(self.dev_labels)))\n",
    "\n",
    "        if test_file:\n",
    "            self.test_contents, self.test_entities1_position, self.test_entities2_position, self.test_labels = self._load_dataset(\n",
    "                test_file)\n",
    "            self.logger.info('Test set size: {} titles.'.format(\n",
    "                len(self.test_labels)))\n",
    "        \n",
    "        self.max_content_len = len(max(self.all_contents,key=lambda x:len(x))) + 2\n",
    "\n",
    "\n",
    "        self.raw_test_contents = self.test_contents.copy()\n",
    "\n",
    "        self.categories = sorted(list(set(self.train_labels + self.dev_labels + self.test_labels)))\n",
    "        \n",
    "        labelsMapping = {'Cause-Effect(e1,e2)': 0,\n",
    "                         'Cause-Effect(e2,e1)': 1,\n",
    "                         'Component-Whole(e1,e2)': 2,\n",
    "                         'Component-Whole(e2,e1)': 3,\n",
    "                         'Content-Container(e1,e2)': 4,\n",
    "                         'Content-Container(e2,e1)': 5,\n",
    "                         'Entity-Destination(e1,e2)': 6,\n",
    "                         'Entity-Destination(e2,e1)': 7,\n",
    "                         'Entity-Origin(e1,e2)': 8,\n",
    "                         'Entity-Origin(e2,e1)': 9,\n",
    "                         'Instrument-Agency(e1,e2)': 10,\n",
    "                         'Instrument-Agency(e2,e1)': 11,\n",
    "                         'Member-Collection(e1,e2)': 12,\n",
    "                         'Member-Collection(e2,e1)': 13,\n",
    "                         'Message-Topic(e1,e2)': 14,\n",
    "                         'Message-Topic(e2,e1)': 15,\n",
    "                         'Product-Producer(e1,e2)': 16,\n",
    "                         'Product-Producer(e2,e1)': 17,\n",
    "                         'Other': 18\n",
    "                        }\n",
    "        \n",
    "        self.cat2id = labelsMapping\n",
    "        self.id2cat = dict(zip(range(len(self.categories)), self.categories))\n",
    "\n",
    "        self.num_class = len(self.cat2id)\n",
    "\n",
    "    def _load_dataset(self, data_path):\n",
    "        \"\"\"\n",
    "        Loads the dataset\n",
    "        Args:\n",
    "            data_path: the data file to load\n",
    "        \"\"\"\n",
    "        with open(data_path, mode='r', encoding='utf-8',errors='ignore') as fin:\n",
    "            lines = fin.readlines()\n",
    "            contents, entities1_pos, entities2_pos, relations = [], [], [], []\n",
    "\n",
    "            for i in range(0, len(lines), 4):\n",
    "                relation = lines[i + 1].strip()\n",
    "                question = lines[i].strip().split('\\t')[1][1:-2].lower()\n",
    "                question, e1_begin, e1_end, e2_begin, e2_end = self._process_question(question)\n",
    "                contents.append(question)\n",
    "                entities1_pos.append([e1_begin, e1_end])\n",
    "                entities2_pos.append([e2_begin, e2_end])\n",
    "                relations.append(str(relation))\n",
    "\n",
    "        return contents, entities1_pos, entities2_pos, relations\n",
    "\n",
    "    def _remove_tag(self, x):\n",
    "        x = x.replace('<e1>', '')\n",
    "        x = x.replace('</e1>', '')\n",
    "        x = x.replace('<e2>', '')\n",
    "        x = x.replace('</e2>', '')\n",
    "        return x\n",
    "\n",
    "    def _process_question(self, question):\n",
    "        question = question.replace(\"'\", \" '\")\n",
    "        question = question.replace(\",\", \" ,\")\n",
    "        question = question.replace(\".\", \" .\")\n",
    "        question = question.split(' ')\n",
    "        e1_begin = e1_end = e2_begin = e2_end = 0\n",
    "\n",
    "        for i, item in enumerate(question):\n",
    "            if item.startswith('<e1>'):\n",
    "                e1_begin = i\n",
    "            if item.endswith('</e1>'):\n",
    "                e1_end = i\n",
    "            if item.startswith('<e2>'):\n",
    "                e2_begin = i\n",
    "            if item.endswith('</e2>'):\n",
    "                e2_end = i\n",
    "\n",
    "        question = list(map(self._remove_tag, question))\n",
    "\n",
    "        return question, e1_begin, e1_end, e2_begin, e2_end\n",
    "\n",
    "    # 将位置距离转化为正的标签 \n",
    "    def _map_position(self, x):\n",
    "        '''\n",
    "        clip the postion range:\n",
    "        '''\n",
    "        if x < -self.limit:\n",
    "            return 0\n",
    "        if -self.limit <= x <= self.limit:\n",
    "            return x + self.limit + 1\n",
    "        if x > self.limit:\n",
    "            return self.limit * 2 + 1\n",
    "\n",
    "    def convert_to_ids(self, vocab):\n",
    "        \"\"\"\n",
    "        Convert the tokens to ids\n",
    "        Args:\n",
    "            vocab: the convert vocab\n",
    "        \"\"\"\n",
    "\n",
    "        if self.train_contents:\n",
    "            self.train_contents = [[vocab.convert_to_ids(contents)]\n",
    "                                   for contents in self.train_contents]\n",
    "\n",
    "        if self.dev_contents:\n",
    "            self.dev_contents = [[vocab.convert_to_ids(contents)]\n",
    "                                 for contents in self.dev_contents]\n",
    "            self.logger.info('Dev set size: {} titles.'.format(\n",
    "                len(self.dev_labels)))\n",
    "\n",
    "        if self.test_contents:\n",
    "            self.test_contents = [[vocab.convert_to_ids(contents)]\n",
    "                                  for contents in self.test_contents]\n",
    "\n",
    "    @property\n",
    "    def all_contents(self) -> list:\n",
    "        \"\"\"\n",
    "        Get all data\n",
    "        Args:\n",
    "            the list of all data\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.train_contents + self.dev_contents + self.test_contents\n",
    "\n",
    "    @property\n",
    "    def all_labels(self) -> list:\n",
    "        \"\"\"\n",
    "        Get the all labels\n",
    "        Returns:\n",
    "            the list of all labels\n",
    "        \"\"\"\n",
    "        return self.train_labels + self.dev_labels + self.test_labels\n",
    "\n",
    "    @property\n",
    "    @lru_cache(1)\n",
    "    def size(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the size of all data\n",
    "        Returns:\n",
    "            an integer indicating the size\n",
    "        \"\"\"\n",
    "        return len(self.train_labels + self.dev_labels + self.test_labels)\n",
    "\n",
    "    @property\n",
    "    @lru_cache(1)\n",
    "    def train_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the size of train data\n",
    "        Returns:\n",
    "            the size of train data\n",
    "        \"\"\"\n",
    "        return len(self.train_labels)\n",
    "\n",
    "    @property\n",
    "    @lru_cache(1)\n",
    "    def dev_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the size of dev data\n",
    "        Returns:\n",
    "            the size of dev data\n",
    "        \"\"\"\n",
    "        return len(self.dev_labels)\n",
    "\n",
    "    @property\n",
    "    @lru_cache(1)\n",
    "    def test_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the size of  test data\n",
    "        Returns:\n",
    "            the size of test data\n",
    "        \"\"\"\n",
    "        return len(self.test_labels)\n",
    "\n",
    "    def _dynamic_padding(self, batch_data):\n",
    "        \"\"\"\n",
    "        Dynamically pads the batch_data with pad_id\n",
    "        \"\"\"\n",
    "        pad_content_len = self.max_content_len\n",
    "        batch_data['contents'] = [(ids + [self.pad_id] *(pad_content_len - len(ids)))[:pad_content_len] \n",
    "                                  for ids in batch_data['contents']]\n",
    "        return batch_data\n",
    "\n",
    "\n",
    "    def _one_mini_batch(self, data) -> dict:\n",
    "        \"\"\"\n",
    "        Get one mini batch\n",
    "        Args:\n",
    "            data: all data\n",
    "        Returns:\n",
    "            one batch of data\n",
    "        \"\"\"\n",
    "        batch_data = {\n",
    "            'contents': [list(contents)[0] for contents in data[0]],\n",
    "            'e1_position': [list(contents) for contents in list(data[1])],\n",
    "            'e2_position': [list(contents) for contents in list(data[2])],\n",
    "#             'e1_entities': [],\n",
    "#             'e2_entities': [],\n",
    "            'e1_distance': [],\n",
    "            'e2_distance': [],\n",
    "            'labels': data[3],\n",
    "            'contents_length': []\n",
    "        }\n",
    "        labelId_list = []\n",
    "        for sent_idx, [content, e1_position, e2_position] in enumerate(zip(batch_data['contents'], \n",
    "                                                                         batch_data['e1_position'], \n",
    "                                                                         batch_data['e2_position'])):\n",
    "            \n",
    "            batch_data['contents_length'].append(len(content))\n",
    "#             batch_data['e1_entities']append(content[e1_position[1]]) \n",
    "#             batch_data['e2_entities']append(content[e2_position[1]]) \n",
    "            \n",
    "            batch_data['e1_distance'].append([self._map_position(idx - e1_position[1]) \n",
    "                                              for idx, _ in enumerate(content)])\n",
    "            batch_data['e1_distance'][sent_idx] = (batch_data['e1_distance'][sent_idx] + \n",
    "                                                   [self.limit*2+2] * (self.max_content_len - len(content)))[:self.max_content_len] \n",
    "            \n",
    "            batch_data['e2_distance'].append([self._map_position(idx - e2_position[1]) \n",
    "                                              for idx, _ in enumerate(content)])\n",
    "            batch_data['e2_distance'][sent_idx] = (batch_data['e2_distance'][sent_idx] + \n",
    "                                                   [self.limit*2+2] * (self.max_content_len - len(content)))[:self.max_content_len] \n",
    "            \n",
    "            lid = self.cat2id[batch_data['labels'][sent_idx]]\n",
    "            labelId_list.append(lid)\n",
    "\n",
    "        batch_data['labels'] = tf.keras.utils.to_categorical(labelId_list, num_classes=len(self.categories))\n",
    "\n",
    "        batch_data = self._dynamic_padding(batch_data)\n",
    "\n",
    "        return batch_data\n",
    "\n",
    "    def gen_mini_batches(self, set_name='train', batch_size=256):\n",
    "        \"\"\"\n",
    "        Generate  batches\n",
    "        Args:\n",
    "            set_name: the mode\n",
    "            batch_size: the size of one batch\n",
    "        \"\"\"\n",
    "        if set_name == 'train':\n",
    "            x = self.train_contents\n",
    "            e1_pos = self.train_entities1_position\n",
    "            e2_pos = self.train_entities1_position\n",
    "            y = self.train_labels\n",
    "            shuffle = True\n",
    "        elif set_name == 'dev':\n",
    "            x = self.dev_contents\n",
    "            e1_pos = self.dev_entities1_position\n",
    "            e2_pos = self.dev_entities1_position\n",
    "            y = self.dev_labels\n",
    "            shuffle = False\n",
    "        elif set_name == 'test':\n",
    "            x = self.test_contents\n",
    "            e1_pos = self.test_entities1_position\n",
    "            e2_pos = self.test_entities1_position\n",
    "            y = self.test_labels\n",
    "            shuffle = False\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                'No data set named as {}'.format(setName))\n",
    "\n",
    "        data_len = len(x)\n",
    "        num_batch = int((data_len - 1) / batch_size) + 1\n",
    "\n",
    "        if shuffle:\n",
    "            indices = np.random.permutation(np.arange(data_len))\n",
    "            x_shuffle = np.array(x)[indices]\n",
    "            e1_pos_shuffle = np.array(e1_pos)[indices]\n",
    "            e2_pos_shuffle = np.array(e2_pos)[indices]\n",
    "            y_shuffle = np.array(y)[indices]\n",
    "        else:\n",
    "            x_shuffle = x\n",
    "            e1_pos_shuffle = e1_pos\n",
    "            e2_pos_shuffle = e2_pos\n",
    "            y_shuffle = y\n",
    "\n",
    "        for i in range(num_batch):\n",
    "            start_id = i * batch_size\n",
    "            end_id = min((i + 1) * batch_size, data_len)\n",
    "            yield self._one_mini_batch([x_shuffle[start_id:end_id],\n",
    "                                        e1_pos_shuffle[start_id:end_id],\n",
    "                                        e2_pos_shuffle[start_id:end_id],\n",
    "                                        y_shuffle[start_id:end_id]\n",
    "                                       ])\n",
    "\n",
    "    def get_category(self, idx):\n",
    "        \"\"\"\n",
    "        Get the category corresponding to idx, returns None if idx is not in vocab\n",
    "        Args:\n",
    "            idx: an integer\n",
    "        returns:\n",
    "            a token string or None\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self.id2cat[idx]\n",
    "        except KeyError:\n",
    "            return None\n",
    "\n",
    "    def save(self, id_categories_dict_path, is_id2categories=True):\n",
    "        \"\"\"\n",
    "        Save the needed data\n",
    "        Args:\n",
    "            id_categories_dict_path: save path\n",
    "            is_id2categories: whether to save the dict of id2categories\n",
    "        \"\"\"\n",
    "        with open(id_categories_dict_path, 'wb') as f:\n",
    "            pickle.dump(self.id2cat, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 实际过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "dataSet = RCDataset(train_file='../raw_data/train.txt', \n",
    "                    dev_file='../raw_data/test.txt',\n",
    "                    test_file='../raw_data/test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 词向量和词典"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 加载本地已有的词向量和词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_vocab_words_list = []\n",
    "\n",
    "with open('../raw_data/pretrained_w2v/vocab.txt', mode='r', encoding='utf-8',errors='ignore') as fin:\n",
    "    lines = fin.readlines()\n",
    "    for word in lines:\n",
    "        load_vocab_words_list.append(word.split('\\n')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "word_2x = numpy.load('../raw_data/pretrained_w2v/w2v_50.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(initial_tokens=load_vocab_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.load_pretrained_embeddings(word_2x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 生成词向量和词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vecModel = Vocab.train_word2vec(dataSet.train_contents, base_dir='../data')\n",
    "# vocab = Vocab(initial_tokens=word2vecModel.wv.index2word)\n",
    "# vocab.load_pretrained_embeddings(word2vecModel.wv.vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 将文本转化成ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet.convert_to_ids(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、模型的构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from typing import Union\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "\n",
    "class CNN_att(object):\n",
    "    \"\"\"\n",
    "    Implements the main CNN RelationClassification \n",
    "    \"\"\"\n",
    "    def __init__(self, args, vocab):\n",
    "\n",
    "        # logging\n",
    "        self.logger = logging.getLogger(\"RC\")\n",
    "        self.log_every_n_batch = args.log_every_n_batch\n",
    "\n",
    "        # basic config\n",
    "        self.seq_length = args.seq_length\n",
    "        self.pos_num = args.pos_num\n",
    "        self.num_classes = args.num_classes\n",
    "        self.use_dropout = args.use_dropout\n",
    "        self.pos_embed_dim = args.pos_embed_dim\n",
    "        self.num_filters = args.num_filters\n",
    "        self.filter_list = [3, 4, 5]\n",
    "        self.fc_size = args.fc_size\n",
    "\n",
    "        self.optim_type = args.optim\n",
    "        self.learning_rate = args.learning_rate\n",
    "        self.clip = args.clip\n",
    "        self.weight_decay = args.weight_decay\n",
    "        \n",
    "        # the vocab\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        # needed config\n",
    "        self.input_all_lenth= self.vocab.embed_dim + self.pos_embed_dim*2\n",
    "        self.init_value = tf.truncated_normal_initializer(stddev=0.1)\n",
    "\n",
    "        # session info\n",
    "        sess_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "        sess_config.gpu_options.allow_growth = True\n",
    "        sess_config.gpu_options.per_process_gpu_memory_fraction = 0.1\n",
    "        self.sess = tf.Session(config=sess_config)\n",
    "\n",
    "        self._build_graph()\n",
    "\n",
    "        # save info\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "        # initialize the model\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def _build_graph(self):\n",
    "        \"\"\"\n",
    "        Builds the computation graph with Tensorflow\n",
    "        \"\"\"\n",
    "        start_t = time.time()\n",
    "        self._setup_placeholders()\n",
    "        self._embed()\n",
    "        self._convolution_layer()\n",
    "        self._classify()\n",
    "        self._compute_loss()\n",
    "        self._compute_acc()\n",
    "        self._create_train_op()\n",
    "        print('Time to build graph: {} s'.format(time.time() - start_t))\n",
    "        param_num = sum([np.prod(self.sess.run(tf.shape(v))) for v in self.all_params])\n",
    "        print('There are {} parameters in the model'.format(param_num))\n",
    "        \n",
    "    def _setup_placeholders(self):\n",
    "        \"\"\"\n",
    "        Placeholders\n",
    "        \"\"\"\n",
    "        # 输入和标签\n",
    "        self.inputs_x = tf.placeholder(tf.int32, [None, self.seq_length],\n",
    "                                       name='inputs_x')\n",
    "        self.distant_1 = tf.placeholder(tf.int32, [None, self.seq_length],\n",
    "                                        name=\"dist_e1\")\n",
    "        self.distant_2 = tf.placeholder(tf.int32, [None, self.seq_length],\n",
    "                                        name=\"dist_e2\")\n",
    "        self.labels_y = tf.placeholder(tf.int32, [None, self.num_classes],\n",
    "                                       name='labels_y')\n",
    "        # 词向量dropout保留的神经元比例\n",
    "        self.emb_keep_prob = tf.placeholder(tf.float32, name='emb_keep_prob')\n",
    "        # RNN的dropout保留的神经元比例\n",
    "        self.rnn_keep_prob = tf.placeholder(tf.float32, name='rnn_keep_prob')\n",
    "        # 全连接dropout保留的神经元比例\n",
    "        self.fc_keep_prob = tf.placeholder(tf.float32, name='fc_keep_prob')\n",
    "        # 输入的实际长度\n",
    "        self.inputs_length = tf.placeholder(tf.int32, [None], name='input_length')\n",
    "\n",
    "    # 词向量层\n",
    "    def _embed(self):\n",
    "        \"\"\"\n",
    "        The embedding layer\n",
    "        \"\"\"\n",
    "        with tf.device('/cpu:0'), tf.variable_scope('embeddings'):\n",
    "            self.word_embeddings = tf.get_variable(name='word_embeddings',\n",
    "                                                   shape=[self.vocab.size, self.vocab.embed_dim],\n",
    "                                                   initializer=tf.constant_initializer(self.vocab.embeddings),\n",
    "#                                                    initializer=tf.random_uniform([self.vocab.size, self.vocab.embed_dim], -0.25, 0.25),\n",
    "                                                   trainable=True)\n",
    "            self.dist1_embeddings = tf.get_variable(name=\"pos1_embeddings\",\n",
    "                                                    dtype=tf.float32,\n",
    "                                                    shape=[self.pos_num+1, self.pos_embed_dim])\n",
    "            self.dist2_embeddings = tf.get_variable(name=\"pos2_embeddings\",\n",
    "                                                    dtype=tf.float32,\n",
    "                                                    shape=[self.pos_num+1, self.pos_embed_dim])\n",
    "            \n",
    "            self.x_emb = tf.nn.embedding_lookup(self.word_embeddings, self.inputs_x)\n",
    "            self.dist1_emb  = tf.nn.embedding_lookup(self.dist1_embeddings, self.distant_1)\n",
    "            self.dist2_emb  = tf.nn.embedding_lookup(self.dist2_embeddings, self.distant_2)\n",
    "            \n",
    "            self.input_emb = tf.concat([self.x_emb, self.dist1_emb, self.dist2_emb], axis=-1, \n",
    "                                       name=\"input_emb\")\n",
    "            \n",
    "            self.input_emb = tf.reshape(self.input_emb, [-1, self.seq_length, self.input_all_lenth, 1])\n",
    "            \n",
    "    def _convolution_layer(self):\n",
    "        \"\"\"\n",
    "        The CNN layer\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"convolution_layer\"):\n",
    "            # 这里使用的是1D卷积，卷积窗口是整个词向量加上位置向量的size\n",
    "            w_windows = self.input_all_lenth\n",
    "            \n",
    "            pool_outputs = []\n",
    "            for filter_size in self.filter_list:\n",
    "                with tf.variable_scope('conv-%s' % filter_size):\n",
    "                    cnn_w = tf.get_variable(shape=[filter_size, w_windows, 1, self.num_filters],\n",
    "                                            initializer=self.init_value,\n",
    "                                            name=\"cnn_w\")\n",
    "                    cnn_b = tf.get_variable(shape=[self.num_filters],\n",
    "                                            initializer=tf.constant_initializer(0.1),\n",
    "                                            name=\"cnn_b\")\n",
    "                    conv = tf.nn.conv2d(self.input_emb, cnn_w, strides=[1, 1, self.input_all_lenth, 1], padding=\"SAME\")\n",
    "                    R = tf.nn.relu(tf.nn.bias_add(conv, cnn_b),name=\"R\") # [batch_size, max_len, 1, n_filters]\n",
    "                    \n",
    "                    R_pool = tf.nn.max_pool(R, ksize=[1,self.seq_length,1 , 1],\n",
    "                                            strides=[1,self.seq_length,1, 1], \n",
    "                                            padding=\"SAME\")  # [batch_size, 1, 1, n_filters]\n",
    "                    pool_outputs.append(R_pool)\n",
    "                    \n",
    "            self.x_encode = tf.reshape(tf.concat(pool_outputs, 3), [-1, 3 * self.num_filters]) # [batch_size, 3 * num_filters]\n",
    "\n",
    "    def _classify(self):\n",
    "        \"\"\"\n",
    "        The classify layer\n",
    "        \"\"\"\n",
    "        # 全连接层，后面接dropout以及relu激活\n",
    "        with tf.name_scope(\"score\"):\n",
    "            # 一层全连接\n",
    "            W_fc = tf.get_variable(\"W_fc\", shape=[3*self.num_filters, self.fc_size], \n",
    "                                   initializer=self.init_value)\n",
    "            b_fc = tf.Variable(tf.constant(0.1, shape=[self.fc_size]), name=\"b_fc\")\n",
    "            self.fc = tf.nn.xw_plus_b(self.x_encode, W_fc, b_fc)\n",
    "            if self.use_dropout:\n",
    "                self.fc = tf.nn.dropout(self.fc, self.fc_keep_prob)\n",
    "            self.fc  = tf.nn.relu(self.fc, name=\"fc1\")\n",
    "\n",
    "            # 分类输出层\n",
    "            W_output = tf.get_variable(\"W_output\", shape=[self.fc_size, self.num_classes],\n",
    "                                       initializer=self.init_value)\n",
    "            b_output = tf.Variable(tf.constant(0.1, shape=[self.num_classes]), name=\"b_output\")\n",
    "            self.logits = tf.nn.xw_plus_b(self.fc, W_output, b_output, name=\"fc2\")\n",
    "            self.y_pred_cls = tf.argmax(tf.nn.softmax(self.logits, name='softmax'), 1, name=\"model_pred\")\n",
    "\n",
    "    def _compute_loss(self):\n",
    "        \"\"\"\n",
    "        The loss function\n",
    "        \"\"\"\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits, labels=self.labels_y)\n",
    "        self.loss = tf.reduce_mean(cross_entropy)\n",
    "        \n",
    "        self.all_params = tf.trainable_variables()\n",
    "        \n",
    "        \n",
    "        if self.weight_decay > 0:\n",
    "            self.all_params = tf.trainable_variables()\n",
    "            with tf.variable_scope('l2_loss'):\n",
    "                l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in self.all_params])\n",
    "            self.loss += self.weight_decay * l2_loss\n",
    "        \n",
    "        if self.clip > 0:\n",
    "            self.globle_step = tf.Variable(0,name=\"globle_step\",trainable=False)\n",
    "            self.tvars = tf.trainable_variables()\n",
    "            self.grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, self.tvars), self.clip)\n",
    "            \n",
    "    def _compute_acc(self):\n",
    "        \"\"\"\n",
    "        The acc\n",
    "        \"\"\"\n",
    "        correct_pred = tf.equal(tf.argmax(self.labels_y , 1), self.y_pred_cls)\n",
    "        self.acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    def _create_train_op(self):\n",
    "        \"\"\"\n",
    "        Selects the training algorithm and creates a train operation with it\n",
    "        \"\"\"\n",
    "        if self.optim_type == 'adagrad':\n",
    "            self.optimizer = tf.train.AdagradOptimizer(self.learning_rate)\n",
    "        elif self.optim_type == 'adam':\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        elif self.optim_type == 'rprop':\n",
    "            self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate)\n",
    "        elif self.optim_type == 'sgd':\n",
    "            self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "        else:\n",
    "            raise NotImplementedError('Unsupported optimizer: {}'.format(self.optim_type))\n",
    "        \n",
    "        if self.clip>0:\n",
    "            self.train_op = self.optimizer.apply_gradients(zip(self.grads, self.tvars))\n",
    "        else:\n",
    "            self.train_op = self.optimizer.minimize(self.loss)\n",
    "\n",
    "\n",
    "    def _train_epoch(self, train_batches, emb_keep_prob, rnn_keep_prob, fc_keep_prob):\n",
    "        \"\"\"\n",
    "        Trains the model for a single epoch\n",
    "        Args:\n",
    "            train_batches: iterable batch data for training\n",
    "            emb_keep_prob: float value indicating dropout keep probability of embedding layer\n",
    "            rnn_keep_prob: float value indicating dropout keep probability of rnn layer\n",
    "            fc_keep_prob: float value indicating dropout keep probability of fully-connection layer\n",
    "        \"\"\"\n",
    "        total_num, total_loss, total_acc = 0, 0, 0\n",
    "        log_every_n_batch, n_batch_loss, n_batch_acc = self.log_every_n_batch, 0, 0\n",
    "        for bitx, batch in enumerate(train_batches, 1):\n",
    "            feed_dict = {self.inputs_x: batch['contents'],\n",
    "                         self.distant_1: batch['e1_distance'],\n",
    "                         self.distant_2: batch['e2_distance'],\n",
    "                         self.labels_y: batch['labels'],\n",
    "                         self.emb_keep_prob: emb_keep_prob,\n",
    "                         self.rnn_keep_prob: rnn_keep_prob,\n",
    "                         self.fc_keep_prob: fc_keep_prob,\n",
    "                         self.inputs_length: batch['contents_length']}\n",
    "            _, loss, acc = self.sess.run([self.train_op, self.loss, self.acc], feed_dict)\n",
    "            total_loss += loss * len(batch['contents'])\n",
    "            total_acc += acc * len(batch['contents'])\n",
    "            total_num += len(batch['contents'])\n",
    "            n_batch_loss += loss\n",
    "            n_batch_acc += acc\n",
    "            if log_every_n_batch > 0 and bitx % log_every_n_batch == 0:\n",
    "                print('Average loss and acc from batch {} to {} is {:>6.2} and {:>7.2%}'\n",
    "                      .format(bitx - log_every_n_batch + 1, \n",
    "                              bitx, \n",
    "                              n_batch_loss / log_every_n_batch, \n",
    "                              n_batch_acc / log_every_n_batch))\n",
    "                n_batch_loss = 0\n",
    "                n_batch_acc = 0\n",
    "        return 1.0 * total_loss / total_num, 1.0 * total_acc / total_num\n",
    "\n",
    "    def train(self, data, epochs, batch_size, save_dir, save_prefix, \n",
    "              emb_keep_prob=1.0, rnn_keep_prob=1.0, fc_keep_prob=1.0,\n",
    "              evaluate=True):\n",
    "        \"\"\"\n",
    "        Train the model with data\n",
    "        Args:\n",
    "            data: the RCDataset class \n",
    "            epochs: number of training epochs\n",
    "            batch_size: the size of one mini-batch\n",
    "            save_dir: the directory to save the model\n",
    "            save_prefix: the prefix indicating the model type\n",
    "            emb_keep_prob: float value indicating dropout keep probability of embedding layer\n",
    "            rnn_keep_prob: float value indicating dropout keep probability of rnn layer\n",
    "            fc_keep_prob: float value indicating dropout keep probability of fully-connection layer\n",
    "            evaluate: whether to evaluate the model on dev data after each epoch\n",
    "        \"\"\"\n",
    "        pad_id = self.vocab.get_id(self.vocab.pad_token)\n",
    "        best_acc_val = 0\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            print('\\nTraining the model for epoch {}'.format(epoch))\n",
    "            train_batches = data.gen_mini_batches('train', batch_size)\n",
    "            train_loss, train_acc = self._train_epoch(train_batches, emb_keep_prob, rnn_keep_prob, fc_keep_prob)\n",
    "            print('The {} Epoch average train loss and acc is {:>6.2} and {:>7.2%}'.format(epoch, train_loss, train_acc))\n",
    "\n",
    "            # 验证评估\n",
    "            if evaluate:\n",
    "                self.logger.info('Evaluating the model after epoch {}'.format(epoch))\n",
    "                if data.dev_contents is not None:\n",
    "                    eval_batches = data.gen_mini_batches('dev', batch_size)\n",
    "                    # 计算验证集的f1、loss和accuracy\n",
    "                    f1_val, loss_val, acc_val = self.evaluate(eval_batches, batch_size, data)\n",
    "                    print('Dev eval loss: {:>6.2}'.format(loss_val))\n",
    "                    print('Dev eval acc: {:>7.2%}'.format(acc_val))\n",
    "                    print('Dev eval f1: {:>7.2%}'.format(f1_val))\n",
    "\n",
    "                    if acc_val > best_acc_val:\n",
    "                        self.save(save_dir, save_prefix)\n",
    "                        best_acc_val = acc_val\n",
    "                else:\n",
    "                    self.logger.warning('No dev set is loaded for evaluation in the dataset!')\n",
    "            else:\n",
    "                self.save(save_dir, save_prefix + '_' + str(epoch))\n",
    "\n",
    "    def evaluate(self, eval_batches, batch_size, data, test=False):\n",
    "        \"\"\"\n",
    "        evaluate the model on dev data or test data\n",
    "        Args:\n",
    "            eval_batches: the eval data\n",
    "            data: the TCDataset class \n",
    "            test: whether to choose test mode\n",
    "        \"\"\"\n",
    "        if test:\n",
    "            data_len = data.test_size\n",
    "        else:\n",
    "            data_len = data.dev_size\n",
    "            \n",
    "        num_batch = int((data_len - 1) / batch_size) + 1\n",
    "        num_batch_list = list(range(num_batch))\n",
    "\n",
    "        y_test_cls = np.zeros(shape=data_len, dtype=np.int32)\n",
    "        y_pred_cls = np.zeros(shape=data_len, dtype=np.int32)  # 保存预测结果\n",
    "\n",
    "        total_loss = 0.0\n",
    "        total_acc = 0.0\n",
    "        \n",
    "        for i, batch in enumerate(eval_batches):  # 逐批次处理\n",
    "            start_id = num_batch_list[i] * batch_size\n",
    "            end_id = min((num_batch_list[i]  + 1) * batch_size, data_len)\n",
    "            feed_dict = {self.inputs_x: batch['contents'],\n",
    "                         self.distant_1: batch['e1_distance'],\n",
    "                         self.distant_2: batch['e2_distance'],\n",
    "                         self.labels_y: batch['labels'],\n",
    "                         self.emb_keep_prob: 1.0,\n",
    "                         self.rnn_keep_prob: 1.0,\n",
    "                         self.fc_keep_prob: 1.0,\n",
    "                         self.inputs_length: batch['contents_length']}\n",
    "            \n",
    "            y_pred_cls[start_id:end_id], loss, acc = self.sess.run([self.y_pred_cls, self.loss, self.acc],\n",
    "                                                                   feed_dict=feed_dict)\n",
    "            y_test_cls[start_id:end_id] = np.argmax(batch['labels'], 1)\n",
    "            \n",
    "            batch_len = len(batch['contents'])\n",
    "            total_loss += loss * batch_len\n",
    "            total_acc += acc * batch_len\n",
    "            \n",
    "        # 评估\n",
    "        f1 = f1_score(y_test_cls, y_pred_cls, average=\"macro\")\n",
    "        \n",
    "        if test:\n",
    "            self.cr_report = classification_report(y_test_cls, y_pred_cls, target_names=sorted(set(data.test_labels)))\n",
    "            self.cm_report = confusion_matrix(y_test_cls, y_pred_cls)\n",
    "            self.y_pred_cls = y_pred_cls\n",
    "            \n",
    "            print('Test eval loss: {:>6.2}'.format(total_loss / data_len))\n",
    "            print('Test eval acc: {:>7.2%}'.format(total_acc / data_len))\n",
    "            print('Test eval f1: {:>7.2%}'.format(f1))                    \n",
    "        else:\n",
    "             return f1, total_loss / data_len, total_acc / data_len\n",
    "\n",
    "\n",
    "    def save(self, model_dir, model_prefix):\n",
    "        \"\"\"\n",
    "        Saves the model into model_dir with model_prefix as the model indicator\n",
    "        Args:\n",
    "            model_dir: the save path\n",
    "            model_prefix: the prefix indicating the model type\n",
    "        \"\"\"\n",
    "        self.saver.save(self.sess, os.path.join(model_dir, model_prefix))\n",
    "        self.logger.info('Model saved in {}, with prefix {}.'.format(model_dir, model_prefix))\n",
    "\n",
    "    def restore(self, model_dir, model_prefix):\n",
    "        \"\"\"\n",
    "        Restores the model into model_dir from model_prefix as the model indicator\n",
    "        Args:\n",
    "            model_dir: the load path\n",
    "            model_prefix: the prefix indicating the model type\n",
    "        \"\"\"\n",
    "        self.saver.restore(self.sess, os.path.join(model_dir, model_prefix))\n",
    "        self.logger.info('Model restored from {}, with prefix {}'.format(model_dir, model_prefix))\n",
    "        \n",
    "        \n",
    "    def save_report(self, data, result_dir, save_suffix: Union[datetime, str, None] = None):\n",
    "        \"\"\"\n",
    "        Saves the model into model_dir with model_prefix as the model indicator\n",
    "        Args:\n",
    "            data: the TCDataset class \n",
    "            result_dir: the save path\n",
    "            save_suffix: the suffix \n",
    "        \"\"\"\n",
    "        if save_suffix is None:\n",
    "            save_suffix = datetime.now().strftime('%Y-%m-%d-%H-%M') \n",
    "            \n",
    "        report_data = [] \n",
    "        lines = self.cr_report.split('\\n') \n",
    "        for line in lines[2:-5]:\n",
    "            row = {}\n",
    "            row_data = line.split()\n",
    "            row['class'] = row_data[0]\n",
    "            row['precision'] = float(row_data[1])\n",
    "            row['recall'] = float(row_data[2])\n",
    "            row['f1_score'] = float(row_data[3])\n",
    "            row['support'] = float(row_data[4])\n",
    "            report_data.append(row)\n",
    "        df = pd.DataFrame.from_dict(report_data)\n",
    "        cr_filename = f'{result_dir}/classification_report_{save_suffix}.csv'\n",
    "        df.to_csv(cr_filename, index = False)\n",
    "        \n",
    "        \n",
    "        df = pd.DataFrame(self.cm_report)\n",
    "        df.columns = sorted(set(data.test_labels))\n",
    "        \n",
    "        df = df.rename(index=data.id2cat)\n",
    "        cm_filename = f'{result_dir}/confusion_matrix_{save_suffix}.csv'\n",
    "        df.to_csv(cm_filename)\n",
    "        \n",
    "        # 预测与实际值统计\n",
    "        df = pd.DataFrame(self.y_pred_cls)\n",
    "        df.columns = ['predict']\n",
    "\n",
    "        df['predict'] = df['predict'].apply(lambda x: data.id2cat[x] )\n",
    "        df['label'] = data.test_labels\n",
    "        compare_filename = f'{result_dir}/predictAndlabel_{save_suffix}.csv'\n",
    "        df.to_csv(compare_filename)\n",
    "        \n",
    "    def write_rc_results(self, data, result_dir, save_suffix: Union[datetime, str, None] = None):\n",
    "        df = pd.DataFrame(self.y_pred_cls)\n",
    "        df.columns = ['predict']\n",
    "        df['predict'] = df['predict'].apply(lambda x: data.id2cat[x] )\n",
    "        \n",
    "        if save_suffix is None:\n",
    "            save_suffix = datetime.now().strftime('%Y-%m-%d-%H-%M')            \n",
    "        \n",
    "        results_file = f'{result_dir}/result_{save_suffix}.txt'\n",
    "        \n",
    "        start_no = 8001\n",
    "        with open(results_file, 'w') as f:\n",
    "            for idx, rel in enumerate(df['predict'].tolist()):\n",
    "                f.write('%d\\t%s\\n' % (start_no+idx, rel))\n",
    "            \n",
    "    # Length of the sequence data\n",
    "    @staticmethod\n",
    "    def _length(seq):\n",
    "        relevant = tf.sign(tf.abs(seq))\n",
    "        length = tf.reduce_sum(relevant, reduction_indices=1)\n",
    "        length = tf.cast(length, tf.int32)\n",
    "        return length\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、设置必要的参数并创建图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args(object):\n",
    "    \"\"\"模型配置类\"\"\"\n",
    "    \n",
    "    # 日志配置\n",
    "    log_every_n_batch = 100  # 每多少个batch打印一次日志    \n",
    "    \n",
    "    # 基础配置\n",
    "    # 1. 整体参数\n",
    "    seq_length = dataSet.max_content_len  # 序列长度\n",
    "    pos_num =123 # 位置的map个数\n",
    "    num_classes = dataSet.num_class  # 标签数\n",
    "    use_dropout = True  # 是否开启dropout\n",
    "    # 2. 词向量\n",
    "    pos_embed_dim = 5\n",
    "    dropout_emb_keep_prob = 0.9  # 词向量层dropout保留比例\n",
    "    # 3. CNN参数\n",
    "    num_filters = 100\n",
    "    # 4. 全连接层\n",
    "    fc_size = 100  # 全连接层的输出维度\n",
    "    dropout_fc_keep_prob = 0.5  # 全连接层dropout保留比例\n",
    "        \n",
    "    # 优化算法配置\n",
    "    optim = 'adam'  # 所选的优化算法\n",
    "    learning_rate =1e-4 # 学习率\n",
    "    clip = 5  # 梯度裁剪的限制，当值为0时，不开启梯度裁剪\n",
    "    weight_decay =  0.00005  # L2正则，当值为0时，不开启正则\n",
    "\n",
    "    \n",
    "    # 训练配置\n",
    "    batch_size = 64  # 每批训练大小\n",
    "    num_epochs = 20  # 总迭代轮次\n",
    "    \n",
    "    # 保存配置\n",
    "    save_dir = './checkpoints/'\n",
    "    save_prefix = 'RC_V1'\n",
    "    report_dir = './data'\n",
    "    results_dir = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 消除当前已经建立的静态图\n",
    "import tensorflow.contrib.keras as kr\n",
    "kr.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build graph: 0.5354444980621338 s\n",
      "There are 1221409 parameters in the model\n"
     ]
    }
   ],
   "source": [
    "model = CNN_att(args, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/RC_V1\n"
     ]
    }
   ],
   "source": [
    "model.restore(args.save_dir, args.save_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the model for epoch 1\n",
      "Average loss and acc from batch 1 to 100 is    1.7 and  99.98%\n",
      "The 1 Epoch average train loss and acc is    1.7 and  99.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Complie\\Anconda\\envs\\TF\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev eval loss:    2.9\n",
      "Dev eval acc:  68.97%\n",
      "Dev eval f1:  64.30%\n",
      "\n",
      "Training the model for epoch 2\n",
      "Average loss and acc from batch 1 to 100 is    1.6 and  99.98%\n",
      "The 2 Epoch average train loss and acc is    1.6 and  99.96%\n",
      "Dev eval loss:    2.8\n",
      "Dev eval acc:  68.86%\n",
      "Dev eval f1:  64.54%\n",
      "\n",
      "Training the model for epoch 3\n",
      "Average loss and acc from batch 1 to 100 is    1.6 and  99.97%\n",
      "The 3 Epoch average train loss and acc is    1.6 and  99.96%\n",
      "Dev eval loss:    2.8\n",
      "Dev eval acc:  69.16%\n",
      "Dev eval f1:  64.73%\n",
      "\n",
      "Training the model for epoch 4\n",
      "Average loss and acc from batch 1 to 100 is    1.6 and  99.95%\n",
      "The 4 Epoch average train loss and acc is    1.6 and  99.95%\n",
      "Dev eval loss:    2.8\n",
      "Dev eval acc:  68.79%\n",
      "Dev eval f1:  64.40%\n",
      "\n",
      "Training the model for epoch 5\n",
      "Average loss and acc from batch 1 to 100 is    1.6 and  99.97%\n",
      "The 5 Epoch average train loss and acc is    1.5 and  99.96%\n",
      "Dev eval loss:    2.7\n",
      "Dev eval acc:  69.08%\n",
      "Dev eval f1:  64.59%\n",
      "\n",
      "Training the model for epoch 6\n",
      "Average loss and acc from batch 1 to 100 is    1.5 and  99.92%\n",
      "The 6 Epoch average train loss and acc is    1.5 and  99.92%\n",
      "Dev eval loss:    2.7\n",
      "Dev eval acc:  69.08%\n",
      "Dev eval f1:  64.65%\n",
      "\n",
      "Training the model for epoch 7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-ccadfb9ab1e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_prefix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout_emb_keep_prob\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m             args.dropout_fc_keep_prob)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-55-fdaaafcdf9ce>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, data, epochs, batch_size, save_dir, save_prefix, emb_keep_prob, rnn_keep_prob, fc_keep_prob, evaluate)\u001b[0m\n\u001b[0;32m    280\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nTraining the model for epoch {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m             \u001b[0mtrain_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen_mini_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m             \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memb_keep_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnn_keep_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfc_keep_prob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'The {} Epoch average train loss and acc is {:>6.2} and {:>7.2%}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-55-fdaaafcdf9ce>\u001b[0m in \u001b[0;36m_train_epoch\u001b[1;34m(self, train_batches, emb_keep_prob, rnn_keep_prob, fc_keep_prob)\u001b[0m\n\u001b[0;32m    243\u001b[0m                          \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc_keep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfc_keep_prob\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                          self.inputs_length: batch['contents_length']}\n\u001b[1;32m--> 245\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'contents'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m             \u001b[0mtotal_acc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'contents'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Complie\\Anconda\\envs\\TF\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Complie\\Anconda\\envs\\TF\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Complie\\Anconda\\envs\\TF\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Complie\\Anconda\\envs\\TF\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Complie\\Anconda\\envs\\TF\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Complie\\Anconda\\envs\\TF\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train(dataSet, \n",
    "            args.num_epochs, \n",
    "            args.batch_size, \n",
    "            args.save_dir,\n",
    "            args.save_prefix, \n",
    "            args.dropout_emb_keep_prob, \n",
    "            args.dropout_fc_keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五、测试和结果保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batches = dataSet.gen_mini_batches('test', args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test eval loss:    2.5\n",
      "Test eval acc:  68.27%\n",
      "Test eval f1:  63.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Complie\\Anconda\\envs\\TF\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "D:\\Complie\\Anconda\\envs\\TF\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(test_batches, args.batch_size, dataSet, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_report(dataSet, args.report_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write_rc_results(dataSet, args.results_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
